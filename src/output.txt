nohup: ignoring input
Namespace(batchsize=128, cuda=False, dataroot='../phone/phoneDatasetFinal.csv', debug=False, epochs=800, lr=0.01, manualSeed=None, name=1708, ngpu=0, nhidden_decoder=128, nhidden_encoder=128, ntimestep=10, resume=True, workers=2)
loading checkpoint 50with loss0.011824486777186394
/home/xeno1897/rnn/DA-RNN/src/model.py:249: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  alpha = F.softmax(x.view(-1, self.input_size))
/home/xeno1897/rnn/DA-RNN/src/model.py:330: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  x.view(-1, 2 * self.decoder_num_hidden + self.encoder_num_hidden)).view(-1, self.T))
Epochs:  50  Iterations:  16  Loss:  0.010226137324934825
Model's state_dict:
gamma_x_l.weight 	 torch.Size([7, 7])
gamma_x_l.bias 	 torch.Size([7])
gamma_h_l.weight 	 torch.Size([128, 7])
gamma_h_l.bias 	 torch.Size([128])
encoder_lstm.weight_ih_l0 	 torch.Size([512, 7])
encoder_lstm.weight_hh_l0 	 torch.Size([512, 128])
encoder_lstm.bias_ih_l0 	 torch.Size([512])
encoder_lstm.bias_hh_l0 	 torch.Size([512])
encoder_attn.weight 	 torch.Size([1, 276])
encoder_attn.bias 	 torch.Size([1])
Model's state_dict:
attn_layer.0.weight 	 torch.Size([128, 384])
attn_layer.0.bias 	 torch.Size([128])
attn_layer.2.weight 	 torch.Size([1, 128])
attn_layer.2.bias 	 torch.Size([1])
lstm_layer.weight_ih_l0 	 torch.Size([512, 1])
lstm_layer.weight_hh_l0 	 torch.Size([512, 128])
lstm_layer.bias_ih_l0 	 torch.Size([512])
lstm_layer.bias_hh_l0 	 torch.Size([512])
fc.weight 	 torch.Size([1, 129])
fc.bias 	 torch.Size([1])
fc_final1.weight 	 torch.Size([128, 256])
fc_final1.bias 	 torch.Size([128])
fc_final2.weight 	 torch.Size([1, 128])
fc_final2.bias 	 torch.Size([1])
